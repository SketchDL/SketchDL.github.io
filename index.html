
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Workshop</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="./css/style.css" rel="stylesheet" type="text/css" />
</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="623" align="center" valign="middle"><h3>CVPR 2021, The 1st Workshop on</h3>
      <span class="title">Sketch-Oriented Deep Learning (SketchDL)</span></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h3>Online, June 19th, 2021</h3></td>
    </tr>
  </table>
  <p><img src="./figures/cvpr2021.jpg" width="300" align="middle" /></p>
</div>

</br>


<div class="container">
  <h2>Speakers</h2>
    <div>
      
      <div class="instructor">
        <a href="https://cogtoolslab.github.io/people.html">
            <div class="instructorphoto"><img src="figures/portraits/judy.jpg"></div>
            <div>Dr. Judith Fan<br>Univ. of California, San Diego</div>
        </a>
      </div>
      
      <div class="instructor">
          <a href="https://www.cs.cmu.edu/~junyanz/" >
        <div class="instructorphoto"><img src="figures/portraits/junyan.jpg"></div>
        <div>Dr. Jun-Yan Zhu<br>Carnegie Mellon Univ.</div>
        </a>
      </div>

      
      <div class="instructor">
        <a href="https://petar-v.com/">
            <div class="instructorphoto"><img src="figures/portraits/peta.jpg"></div>
            <div>Dr. Petar Veličković<br>DeepMind</div>
        </a>
      </div>

      
    </div>
    <p></p> 
</div>   


</br>




<div class="container">
  <h2>Organizers</h2>
    <div>
      <div class="instructor">
          <a href="http://www.pengxu.net/" >
        <div class="instructorphoto"><img src="figures/portraits/peng.jpg"></div>
        <div>Peng Xu</div>
        </a>
                
      </div>

      <div class="instructor">
        <a href="https://otoro.net/ml/">
            <div class="instructorphoto"><img src="figures/portraits/david.jpg"></div>
            <div>David Ha</div>
        </a>
        
      </div>

      <div class="instructor">
        <a href="https://www.surrey.ac.uk/people/john-collomosse">
            <div class="instructorphoto"><img src="figures/portraits/john.jpg"></div>
            <div>John Collomosse</div>
        </a>
        
      </div>

      <div class="instructor">
        <a href="https://www.cl.cam.ac.uk/~aco41/">
            <div class="instructorphoto"><img src="figures/portraits/cengiz.JPG"></div>
            <div>Cengiz Öztireli</div>
        </a>
        
      </div>
      
      
      
      <div class="instructor">
          <a href="https://sweb.cityu.edu.hk/hongbofu/" >
        <div class="instructorphoto"><img src="figures/portraits/hongbo.jpg"></div>
        <div>Hongbo Fu</div>
        </a>
                
      </div>

      <div class="instructor">
          <a href="https://iui.ku.edu.tr/people/" >
        <div class="instructorphoto"><img src="figures/portraits/metin.jpg"></div>
        <div>Metin Sezgin</div>
        </a>
                
      </div>

      <div class="instructor">
          <a href="https://people.inf.ethz.ch/sorkineo/" >
        <div class="instructorphoto"><img src="figures/portraits/olga.jpg"></div>
        <div>Olga Sorkine-Hornung</div>
        </a>
                
      </div>

      <div class="instructor">
          <a href="https://www.cc.gatech.edu/~hays/" >
        <div class="instructorphoto"><img src="figures/portraits/james.jpg"></div>
        <div>James Hays</div>
        </a>
                
      </div>
      
    </div>
 
</div>   

</br>

<div class="container">
  <h2>Overview</h2>
    <p style="text-align:justify; text-justify:inter-ideograph;">Drawing is a universal communication method that transcends barriers to link human societies. It has been used from ancient times to today, comes naturally to children before writing, and transcends language barriers. The recent prevalence of touchscreen devices has made sketch creation a much easier task than ever and consequently made sketch-oriented applications increasingly popular, e.g., Quick!Draw online game. The progress of deep learning and machine learning has immensely benefited sketch research and applications, e.g., GAN (generative adversarial networks), GNN (graph neural network), meta-learning, self-supervised learning. Moreover, large-scale (even million-scale) sketch datasets have been already emerged in recent years. The proliferation of mobile computing devices with touchscreens interfaces have also sparked interest in machine learning methods that can process human sketching–whether as an interface with our devices, or to facilitate content production and communication of ideas. All this is bringing new opportunities and challenges to the field of sketch-oriented research.
    </p>

    <p style="text-align:justify; text-justify:inter-ideograph;">This workshop aims to bring researchers together from a diverse scope of research areas (e.g., computer vision, computer graphics, human computer interaction, deep learning, machine learning, cognitive science), to explore directions and topics for future sketch-oriented machine learning.
    </p>
</div>

</br>

<div class="container">
  <h2>Program</h2>
    <div class="schedule">
      <p><span class="announce_date">08:45 - 09:00 </span>.  opening remarks </p>
      <p><span class="announce_date">09:00 - 09:40 </span>.  keynote by <b>Dr. Jun-Yan Zhu</b>, "Image Projection and Editing with GANs" </p>
      <p><span class="announce_date">09:40 - 10:20 </span>.  keynote by <b>Dr. Petar Veličković</b>,  "Graph Representation Learning and Potential Applications for Sketch" </p>
      <p><span class="announce_date">10:20 - 10:50 </span>.  break & poster & demo session </p>
      <p><span class="announce_date">10:50 - 11:30 </span>.  keynote by <b>Dr. Judith Fan</b>, "Cognitive Tools for Making the Invisible Visible" </p>
      <p><span class="announce_date">11:30 - 11:50 </span>.  oral paper presentation </p>
      <p><span class="announce_date">11:50 - 12:10 </span>.  oral paper presentation </p>
      <p><span class="announce_date">12:10 - 12:30 </span>.  oral paper presentation </p>
      <p><span class="announce_date">12:30 - 12:50 </span>.  oral paper presentation </p>
      <p><span class="announce_date">12:50 - 13:30 </span>.  panel discussion for open problem </p>
      <p><span class="announce_date">13:30 - 13:45 </span>.  closing remark, awarding best paper </p>
    </div>
</div>


</br>

<div class="container">
  <h2>Call for Paper</h2>
  <p>(CFP poster can be downloaded via this <a href="figures/poster.jpg">link</a>)</p>

    <h3>Topics of Interests</h3>

    <div align="center">
      <p style="text-align:justify; text-justify:inter-ideograph;">This workshop encourages novel and creative deep learning works for all forms of drawings, including free-hand sketch, professional (forensic) facial sketch, professional pencil sketch, professional landscape sketch, cartoon/manga, well-drawn 3D sketch, etc.
      </p>
      
      
      <p style="text-align:justify; text-justify:inter-ideograph;">Topics of interests by this workshop include, but are not limited to:</p>

      <div align="left">
        <ul>

          <li><b>uni-modal tasks</b>

            <ul>

              <li>global-level understanding and interpretation

                <ul>
                  <li>sketch object/scene/face recognition</li>
                  <li>sketch online/offline recognition</li>
                  <li>sketch retrieval and hashing</li>
                  <li>sketch generation</li>
                  <li>sketch-oriented neural representation, e.g., CNN- or RNN- network designing for sketch</li>
                </ul>

              </li>

              <li>partial-level understanding and interpretation

                <ul>
                  <li>sketch grouping</li>
                  <li>sketch segmentation (pixel-level parse and stroke-level segmentation)</li>
                  <li>sketch abstraction</li>
                </ul>

              </li>

            </ul>

          </li>

          <li><b>multi-modal tasks</b>

            <ul>
              <li>sketch related cross-modal visual retrieval/hashing, e.g., sketch-based photo/video/3D retrieval</li>
              <li>sketch related cross-modal generation</li>
              <li>text to sketch generation</li>
              <li>applications with other modalities, e.g., spatial text, art clip, cartoon</li>
            </ul>
          </li>

          <li><b>emerging and potential theory and applications</b>

            <ul>
              <li>sketch-oriented GNN/GCN (graph neural/convolutional network) and TCN (temporal/textual convolutional neural
network) designing and representation</li>
              <li>self-supervised/un-supervised/weakly-supervised learning and representation for sketch</li>
              <li>transfer learning, meta learning, zero-shot and few-shot learning for sketch</li>
              <li>adversarial learning for sketch</li>
              <li>sketch-related security surveillance, e.g., sketch-based person Re-ID, sketch-based forensic applications</li>
              <li>sketch-related AR/VR (Augmented Reality and Virtual Reality) and HCI applications</li>
              <li>sketch-related RL (Reinforcement Learning)</li>
              <li>sketch for computer graphics, robots, art/industry design, business, education</li>
            </ul>

          </li>

        </ul>
      </div>
    </div>


    <h3>Important Dates</h3>
      <div align="left">
        <p><span class="announce_date">Paper Submission Deadline </span>.  Mar. 23, 2021 </p>
        <p><span class="announce_date">Notification of Acceptance </span>.  Apr. 15, 2021 </p>
        <p><span class="announce_date">Camera-ready Due </span>.  Apr. 20, 2021 </p>
      </div>


    <h3>Submission and Review</h3>
    <div align="center">
      <p style="text-align:justify; text-justify:inter-ideograph;">All submissions will be handled electronically via the workshop’s CMT Website. Link to the submission site will come soon.
      </p>

      <p style="text-align:justify; text-justify:inter-ideograph;"> All submissions will undergo standard double-blind peer-review.
      </p>

      <p style="text-align:justify; text-justify:inter-ideograph;">Length, format, and template should follow the <a href="http://cvpr2021.thecvf.com/node/33#submission-guidelines">CVPR 2021 Submission Guidelines</a>.
      </p>
      
      <p style="text-align:justify; text-justify:inter-ideograph;">The <b>best paper award</b> will be sponsored by Google.
      </p>
    </div>


</div>


</br>



<div class="container">
  <h2>Technical Program Committee Members</h2>
    <div align="left">
      <p><a href="https://www.brialong.com/">Bria Long</a>, Stanford University</p>
      <p><a href="https://cusuh.github.io/">Cusuh Ham</a>, Georgia Institute of Technology</p>
      <p><a href="https://scholar.google.com/citations?user=NMMB7wcAAAAJ&hl=en">Kun Liu</a>, Beijing Univ. of Posts & Telecommunications</p>
      <p><a href="https://scholar.google.com.br/citations?user=vunj2dMAAAAJ&hl=en">Leo Sampaio Ferraz Ribeiro</a>, Universidade de São Paulo</p>
      <p><a href="">Manfred Lau</a>, City University of Hong Kong</p>
      <p><a href="">Mengqiu Xu</a>, Beijing Univ. of Posts & Telecommunications</p>
      <p><a href="https://sites.google.com/site/moacirponti/">Moacir Antonelli Ponti</a>, Universidade de São Paulo</p>
      <p><a href="https://www.cc.gatech.edu/~psangklo/">Patsorn Sangkloy</a>, Georgia Institute of Technology</p>
      <p><a href="https://scholar.google.com/citations?user=cWvu-I8AAAAJ&hl=en">Pengkai Zhu</a>, Boston University</p>
      <p><a href="https://github.com/qyzdao">Qingyuan Zheng</a>, University of Maryland</p>
      <p><a href="">Tongtong Yuan</a>, Beijing University of Technology</p>
      <p><a href="">Xiaoguang Han</a>, The Chinese University of Hong Kong (ShenZhen)</p>
      <p><a href="">Xiaoying Feng</a>, Avar Consulting, Inc. & American Institutes for Research</p>
      <p><a href="https://xiatian-zhu.github.io/">Xiatian Zhu</a>, Samsung AI Centre, UK</p>
      <p><a href="">Yongye Huang</a>, ByteDance</p>
      <p><a href="">Youyi Zheng</a>, Zhejiang University</p>
    </div>
</div>

</br>

<div class="container">
  <h2>Sponsor</h2>
    <div>
      
      <div id="special" class="instructor">
        <a href="https://www.google.com">
            <div class="instructorphoto"><img src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=2974331913,2785783542&fm=26&gp=0.jpg"></div>
        </a>
      </div>

      
    </div>
    <p></p> 
</div>   


</br>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=E6np3JRFC4WCDjj7DPsJgF_ArylvOLfpcVUOTmedD6Q&cl=ffffff&w=a"></script>

</br>

<div class="containersmall">
  <p>The webpage template is from <a href="https://gkioxari.github.io/">here</a>.</p>
</div>
 
<!--<p align="center" class="acknowledgement">Last updated: 30 July 2012</p>-->
</body>
</html>
